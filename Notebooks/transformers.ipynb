{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002ffd23-8e36-46c0-aff0-811b7ba476f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /Users/users/mahesh/.local/lib/python3.11/site-packages (1.0.15)\n",
      "Requirement already satisfied: torch in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from timm) (2.4.0)\n",
      "Requirement already satisfied: torchvision in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from timm) (0.19.0)\n",
      "Requirement already satisfied: pyyaml in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /Users/users/mahesh/.local/lib/python3.11/site-packages (from timm) (0.29.3)\n",
      "Requirement already satisfied: safetensors in /Users/users/mahesh/.local/lib/python3.11/site-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: filelock in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from huggingface_hub->timm) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from huggingface_hub->timm) (23.1)\n",
      "Requirement already satisfied: requests in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from huggingface_hub->timm) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from huggingface_hub->timm) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from torch->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from torch->timm) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from torch->timm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from torch->timm) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from torch->timm) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from torch->timm) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from torch->timm) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from torch->timm) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from torch->timm) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from torch->timm) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.6.20)\n",
      "Requirement already satisfied: numpy in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from torchvision->timm) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from jinja2->torch->timm) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2025.1.31)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a6af88b-a93e-427c-8b07-39596e43986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip show timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c24fdeb3-1ee6-4245-92aa-3acbf7f68aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f80c9a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/net/virgo01/data/users/mahesh/DeepLearning/DLP_galaxy_mergers/Notebooks', '/Software/users/modules/9/software/anaconda3/2024.02/lib/python311.zip', '/Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11', '/Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/lib-dynload', '', '/Users/users/mahesh/.local/lib/python3.11/site-packages', '/Software/users/modules/9/software/anaconda3/2024.02/lib/python3.11/site-packages', '/net/virgo01/data/users/mahesh/DeepLearning/DLP_galaxy_mergers/Scripts']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/virgo01/data/users/mahesh/DeepLearning/DLP_galaxy_mergers/Scripts/AkhilFunctions.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True good\n",
      "1 devices\n",
      "(24953159680, 25339101184)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "sys.path.append('/net/virgo01/data/users/mahesh/DeepLearning/DLP_galaxy_mergers/Scripts')\n",
    "print(sys.path)\n",
    "import DataCore_Akhil as DC\n",
    "import AkhilFunctions as AF\n",
    "import auxiliary_functions as af\n",
    "import plotting\n",
    "\n",
    "import torch\n",
    "print(f\"{torch.cuda.is_available()} good\")\n",
    "print(f\"{torch.cuda.device_count()} devices\")\n",
    "print(torch.cuda.mem_get_info())\n",
    "\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, TensorDataset, Subset, ConcatDataset\n",
    "from astropy.io import fits\n",
    "import torchvision\n",
    "import timm\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf251c37-2474-4ff4-8fb6-47bffaf24fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05086d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/net/virgo01/data/users/mahesh/DeepLearning/data/\"\n",
    "labeldir = \"/net/virgo01/data/users/spirov/Deep/catalog_tng100_jwst_all_50sns.fits\"\n",
    "labels = fits.open(labeldir)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eccb067d-82a6-45d2-8cb2-ebe5c6f368cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = labels.data['is_major_merger'] == 0\n",
    "mask2 = labels.data['is_pre_merger'] == 0\n",
    "mask3 = labels.data['is_ongoing_merger'] == 1\n",
    "mask4 = labels.data['is_post_merger'] == 0\n",
    "#print(len(labels.data[mask1 & mask2 & mask3 & mask4]))\n",
    "#print(len(labels.data[mask1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18cc214-1ed0-493c-83ef-b19743419ad0",
   "metadata": {},
   "source": [
    "major: 2383\n",
    "\n",
    "pre only: 1236\\\n",
    "ongoing only: 511\\\n",
    "post only: 605\\\n",
    "pre and post: 31\\\n",
    "sum: 2383\n",
    "31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3aa12b-2af4-4afe-baa7-36321b1956bc",
   "metadata": {},
   "source": [
    "# Multi-target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d909dd8-532a-4e42-a59b-4af0884897ea",
   "metadata": {},
   "source": [
    "## DeiT III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccad7fe-4ddf-4277-9e06-2471bb2424d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deit3_multi = timm.create_model(\"deit3_base_patch16_224\", pretrained = True)\n",
    "num_classes = 2\n",
    "deit3_multi.head = nn.Linear(deit3_multi.head.in_features, num_classes)\n",
    "\n",
    "# Freeze all parameters first\n",
    "for param in deit3_multi.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the classification head\n",
    "if hasattr(deit3_multi, \"head\"):\n",
    "    for param in deit3_multi.head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Unfreeze the last num_unfreeze transformer blocks\n",
    "num_unfreeze = 6\n",
    "if hasattr(deit3_multi, \"blocks\"):\n",
    "    for block in deit3_multi.blocks[-num_unfreeze:]:\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "deit3_multi = deit3_multi.to(device)\n",
    "\n",
    "config = timm.data.resolve_model_data_config(deit3_multi)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.squeeze(0) if img.shape[0] == 1 else img),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(config[\"input_size\"][1:]),  # Resize to model's expected input size\n",
    "    #transforms.RandomResizedCrop(256, scale=(0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=(0, 360)),\n",
    "    transforms.Grayscale(num_output_channels=3),   # Convert grayscale to 3 channels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: AF.aggressive_arcsinh(t)),\n",
    "    transforms.Normalize(mean=config[\"mean\"], std=config[\"std\"])  # Use model-specific normalization\n",
    "])\n",
    "\n",
    "multidata = DC.ClassificationDataset(datadir, labels, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72966b8f-5440-48f2-bc89-5cb8b8142ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation/test transform without augmentations\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.squeeze(0) if img.shape[0] == 1 else img),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(config[\"input_size\"][1:]),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: aggressive_arcsinh(t)),\n",
    "    transforms.Normalize(mean=config[\"mean\"], std=config[\"std\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888225f8-ea68-488f-b060-3716875da671",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(deit3_multi.blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b495390-7e5f-48fb-88b3-b8860031ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_labels = np.empty((len(labels.data), 2), dtype = int)\n",
    "mask_pre = labels.data['is_pre_merger'] == 1\n",
    "mask_post = labels.data['is_post_merger'] == 1\n",
    "mask_ongoing = labels.data['is_ongoing_merger'] == 1\n",
    "mask_non = labels.data['is_major_merger'] == 0\n",
    "\n",
    "strat_labels[mask_pre, :] = [1,0]\n",
    "strat_labels[mask_post | mask_ongoing, :] = [0,1]\n",
    "strat_labels[mask_non, :] = [0,0]\n",
    "strat_labels[mask_pre & mask_post] = [1,1]\n",
    "\n",
    "print(len(strat_labels[mask_pre]))\n",
    "print(len(strat_labels[mask_post | mask_ongoing]))\n",
    "print(len(strat_labels[mask_pre & mask_post]))\n",
    "print(len(strat_labels[mask_non]))\n",
    "print('')\n",
    "print(np.sum((strat_labels == [1,0]).all(axis=1)))\n",
    "print(np.sum((strat_labels == [0,1]).all(axis=1)))\n",
    "print(np.sum((strat_labels == [1,1]).all(axis=1)))\n",
    "print(np.sum((strat_labels == [0,0]).all(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485df75c-fcda-4014-ae57-f0923cea25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = np.arange(len(multidata))\n",
    "\n",
    "# First split: 80% train, 20% temporary (which will later be split into val and test)\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    all_indices, test_size=0.20, stratify=strat_labels, random_state=42\n",
    ")\n",
    "\n",
    "train_strat_labels = strat_labels[train_idx]\n",
    "first_train_strat_labels = train_strat_labels\n",
    "\n",
    "desample_factor = 0.8\n",
    "n00 = desample_factor*np.sum((train_strat_labels == [0,0]).all(axis=1))  # count for (0,0)\n",
    "augmentation_factor = 2  #times x as augmenting with x times minority classes\n",
    "n10 = augmentation_factor*np.sum((train_strat_labels == [1,0]).all(axis=1))  # count for (1,0)\n",
    "n01 = augmentation_factor*np.sum((train_strat_labels == [0,1]).all(axis=1))  # count for (0,1)\n",
    "n11 = augmentation_factor*np.sum((train_strat_labels == [1,1]).all(axis=1))  # count for (1,1)\n",
    "\n",
    "pos_weight_first = (n00 + n01) / (n10 + n11)\n",
    "pos_weight_second = (n00 + n10) / (n01 + n11)\n",
    "\n",
    "pos_weight = torch.tensor([pos_weight_first, pos_weight_second], dtype=torch.float32, device=device)\n",
    "\n",
    "# For the temp set, stratify again. Extract stratification labels for the temp indices.\n",
    "temp_strat_labels = strat_labels[temp_idx]\n",
    "\n",
    "# Second split: split temp indices equally into validation and test sets\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx, test_size=0.5, stratify=temp_strat_labels, random_state=42\n",
    ")\n",
    "\n",
    "# Create the Subset datasets\n",
    "multi_train_dataset = Subset(multidata, train_idx)\n",
    "# multi_val_dataset = Subset(multidata, val_idx)\n",
    "# multi_test_dataset = Subset(multidata, test_idx)\n",
    "\n",
    "multi_val_dataset = AF.SubsetWithTransform(multidata, val_idx, transform=val_test_transform)\n",
    "multi_test_dataset = AF.SubsetWithTransform(multidata, test_idx, transform=val_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6566eaf1-d602-44a0-a031-8b89db419f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = np.arange(len(multi_train_dataset))\n",
    "\n",
    "# Create a boolean mask for the majority class (0,0)\n",
    "mask_majority = np.all(train_strat_labels == [0,0], axis=1)\n",
    "majority_indices = all_indices[mask_majority]\n",
    "\n",
    "# Downsample the (0,0) samples to 80% of their original count\n",
    "new_majority_indices = np.random.choice(majority_indices, \n",
    "                                          size=int(0.8 * len(majority_indices)), \n",
    "                                          replace=False)\n",
    "\n",
    "# For the minority classes, keep all indices\n",
    "minority_indices = all_indices[~mask_majority]\n",
    "\n",
    "# Combine the indices and optionally shuffle them\n",
    "new_indices = np.concatenate([new_majority_indices, minority_indices])\n",
    "np.random.shuffle(new_indices)\n",
    "\n",
    "train_strat_labels = train_strat_labels[new_indices]\n",
    "\n",
    "# Create a new Subset dataset with the new indices\n",
    "multi_train_dataset = Subset(multi_train_dataset, new_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c4424d-d8ba-4023-abb3-39b8a3c36efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(multi_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8ce6af-b388-42dd-9f8e-b3ec770f3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for minority samples (i.e., not [0,0])\n",
    "minority_mask = ~np.all(train_strat_labels == [0,0], axis=1)\n",
    "minority_indices = np.where(minority_mask)[0].tolist()\n",
    "\n",
    "# Create a subset for the minority samples\n",
    "minority_dataset = Subset(multi_train_dataset, minority_indices)\n",
    "\n",
    "# Concatenate the original dataset with the minority subset (doubling the minority samples)\n",
    "multi_train_dataset = ConcatDataset([multi_train_dataset, minority_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca9482-37ad-4564-9136-cb5027566ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(multi_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06597c22-7097-43c1-9ed0-8ac5b635541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = int(0.81 * len(multidata))\n",
    "# val_size = int(0.09*len(multidata))\n",
    "# test_size = len(multidata) - train_size - val_size\n",
    "# multi_train_dataset, multi_val_dataset, multi_test_dataset = random_split(multidata, [train_size, val_size, test_size])\n",
    "\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "prefetch_factor = 2\n",
    "persistent_workers = True\n",
    "multi_train_loader = DataLoader(multi_train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "                          pin_memory=True, persistent_workers=persistent_workers, prefetch_factor = prefetch_factor)\n",
    "multi_val_loader = DataLoader(multi_val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                        pin_memory=True, persistent_workers=persistent_workers, prefetch_factor = prefetch_factor)\n",
    "multi_test_loader = DataLoader(multi_test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                         pin_memory=True, persistent_workers=persistent_workers, prefetch_factor = prefetch_factor)\n",
    "\n",
    "x,y = next(iter(multi_train_loader))\n",
    "print(\"x batch shape:\", x.shape)  # Should be [batch_size, 3, 224, 224]\n",
    "print(\"y batch shape:\", y.shape)  # Should be [batch_size, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a3bc4a-801a-478b-bbd7-f6a7f5ab6a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "#optimizer = optim.Adam(deit3_multi.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "optimizer = optim.AdamW(deit3_multi.parameters(),\n",
    "                        lr=1e-3,            # learning rate, adjust as needed\n",
    "                        betas=(0.9, 0.999), # momentum parameters\n",
    "                        eps=1e-8,           # term added to improve numerical stability\n",
    "                        weight_decay=0.01)  # decoupled weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c13e6c-ba9b-488d-9d58-1acab88647c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "deit3_multi_train_losses = []\n",
    "deit3_multi_val_losses = []\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "    train_loss = AF.training_epoch(deit3_multi, multi_train_loader, optimizer, criterion, device, unsqueezeY = False)\n",
    "    val_loss = AF.evaluation_epoch(deit3_multi, multi_val_loader, criterion, device, desc = 'validation', unsqueezeY = False)\n",
    "\n",
    "    # Step the scheduler\n",
    "    #scheduler.step()\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    deit3_multi_train_losses.append(train_loss)\n",
    "    deit3_multi_val_losses.append(val_loss)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}/{epochs} took {epoch_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "deit3_multi_epoch_loss, deit3_multi_epoch_acc, deit3_multi_all_preds, deit3_multi_all_labels = AF.multilabel_evaluate(deit3_multi, multi_test_loader,\n",
    "                                                                                                                  criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6c661-1976-4649-b8a7-5a539e8ea796",
   "metadata": {},
   "outputs": [],
   "source": [
    "if epoch+1 == epochs:\n",
    "    ep = epochs\n",
    "else:\n",
    "    ep = epoch\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(np.arange(ep), deit3_multi_train_losses, label=\"Training\")\n",
    "ax.plot(np.arange(ep), deit3_multi_val_losses, label=\"Validation\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752b154e-e4f5-48eb-b7a2-b774ba304240",
   "metadata": {},
   "outputs": [],
   "source": [
    "af.multilabel_plot_confusion_matrix(deit3_multi_all_labels, deit3_multi_all_preds, classes=None, normalize=True, title=None, cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785052d6-379f-49de-bafd-f8cbfaa14581",
   "metadata": {},
   "source": [
    "## Swin V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3929e4d6-322c-481a-9e5e-747f0de685a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_multi = timm.create_model(\"swinv2_base_window16_256\", pretrained = True)\n",
    "num_classes = 2\n",
    "#swin_multi.head = nn.Linear(swin_multi.head.in_features, num_classes)\n",
    "swin_multi.reset_classifier(num_classes, global_pool='avg')\n",
    "\n",
    "# Freeze all parameters first\n",
    "for param in swin_multi.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the classification head\n",
    "if hasattr(swin_multi, \"head\"):\n",
    "    for param in swin_multi.head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Unfreeze the last num_unfreeze layers\n",
    "num_unfreeze = 1\n",
    "for layers in swin_multi.layers[-num_unfreeze:]:\n",
    "        for param in layers.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "swin_multi = swin_multi.to(device)\n",
    "\n",
    "config = timm.data.resolve_model_data_config(swin_multi)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.squeeze(0) if img.shape[0] == 1 else img),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(config[\"input_size\"][1:]),  # Resize to model's expected input size\n",
    "    #transforms.RandomResizedCrop(256, scale=(0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=(0, 360)),\n",
    "    transforms.Grayscale(num_output_channels=3),   # Convert grayscale to 3 channels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: AF.aggressive_arcsinh(t)),\n",
    "    transforms.Normalize(mean=config[\"mean\"], std=config[\"std\"])  # Use model-specific normalization\n",
    "])\n",
    "\n",
    "swin_multidata = DC.ClassificationDataset(datadir, labels, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470e4c2e-4e9d-47fa-b928-b1f7245ab8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(swin_multi.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0a8c7f-11e4-4e07-8563-cb30b3d774d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = np.arange(len(swin_multidata))\n",
    "\n",
    "# First split: 80% train, 20% temporary (which will later be split into val and test)\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    all_indices, test_size=0.20, stratify=strat_labels, random_state=42\n",
    ")\n",
    "\n",
    "train_strat_labels = strat_labels[train_idx]\n",
    "#first_train_strat_labels = train_strat_labels\n",
    "\n",
    "desample_factor = 0.8\n",
    "n00 = desample_factor*np.sum((train_strat_labels == [0,0]).all(axis=1))  # count for (0,0)\n",
    "augmentation_factor = 2  #times x as augmenting with x times minority classes\n",
    "n10 = augmentation_factor*np.sum((train_strat_labels == [1,0]).all(axis=1))  # count for (1,0)\n",
    "n01 = augmentation_factor*np.sum((train_strat_labels == [0,1]).all(axis=1))  # count for (0,1)\n",
    "n11 = augmentation_factor*np.sum((train_strat_labels == [1,1]).all(axis=1))  # count for (1,1)\n",
    "\n",
    "pos_weight_first = (n00 + n01) / (n10 + n11)\n",
    "pos_weight_second = (n00 + n10) / (n01 + n11)\n",
    "\n",
    "pos_weight = torch.tensor([pos_weight_first, pos_weight_second], dtype=torch.float32, device=device)\n",
    "\n",
    "# For the temp set, stratify again. Extract stratification labels for the temp indices.\n",
    "temp_strat_labels = strat_labels[temp_idx]\n",
    "\n",
    "# Second split: split temp indices equally into validation and test sets\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx, test_size=0.5, stratify=temp_strat_labels, random_state=42\n",
    ")\n",
    "\n",
    "swin_multi_train_dataset = Subset(swin_multidata, train_idx)\n",
    "# swin_multi_val_dataset = Subset(swin_multidata, val_idx)\n",
    "# swin_multi_test_dataset = Subset(swin_multidata, test_idx)\n",
    "\n",
    "swin_multi_val_dataset = AF.SubsetWithTransform(swin_multidata, val_idx, transform=val_test_transform)\n",
    "swin_multi_test_dataset = AF.SubsetWithTransform(swin_multidata, test_idx, transform=val_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3fa8f3-cce5-4e0e-8a77-65caf978ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = np.arange(len(swin_multi_train_dataset))\n",
    "\n",
    "# Create a boolean mask for the majority class (0,0)\n",
    "mask_majority = np.all(train_strat_labels == [0,0], axis=1)\n",
    "majority_indices = all_indices[mask_majority]\n",
    "\n",
    "# Downsample the (0,0) samples to 80% of their original count\n",
    "new_majority_indices = np.random.choice(majority_indices, \n",
    "                                          size=int(0.8 * len(majority_indices)), \n",
    "                                          replace=False)\n",
    "\n",
    "# For the minority classes, keep all indices\n",
    "minority_indices = all_indices[~mask_majority]\n",
    "\n",
    "# Combine the indices and optionally shuffle them\n",
    "new_indices = np.concatenate([new_majority_indices, minority_indices])\n",
    "np.random.shuffle(new_indices)\n",
    "\n",
    "train_strat_labels = train_strat_labels[new_indices]\n",
    "\n",
    "# Create a new Subset dataset with the new indices\n",
    "swin_multi_train_dataset = Subset(swin_multi_train_dataset, new_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152a074-6147-4d17-bcd6-066f9ae584d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(swin_multi_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e979586-86fe-40ea-9e25-ee53123cd9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for minority samples (i.e., not [0,0])\n",
    "minority_mask = ~np.all(train_strat_labels == [0,0], axis=1)\n",
    "minority_indices = np.where(minority_mask)[0].tolist()\n",
    "\n",
    "# Create a subset for the minority samples\n",
    "minority_dataset = Subset(swin_multi_train_dataset, minority_indices)\n",
    "\n",
    "# Concatenate the original dataset with the minority subset (doubling the minority samples)\n",
    "swin_multi_train_dataset = ConcatDataset([swin_multi_train_dataset, minority_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd752a9-035f-481d-a0a9-19d3c3eb87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(swin_multi_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6387c56-3539-44c6-b819-29b1449feba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = int(0.81 * len(swin_multidata))\n",
    "# val_size = int(0.09*len(swin_multidata))\n",
    "# test_size = len(swin_multidata) - train_size - val_size\n",
    "# swin_multi_train_dataset, swin_multi_val_dataset, swin_multi_test_dataset = random_split(swin_multidata, [train_size, val_size, test_size])\n",
    "\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "prefetch_factor = 2\n",
    "persistent_workers = True\n",
    "swin_multi_train_loader = DataLoader(swin_multi_train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "                          pin_memory=True, persistent_workers=persistent_workers, prefetch_factor = prefetch_factor)\n",
    "swin_multi_val_loader = DataLoader(swin_multi_val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                        pin_memory=True, persistent_workers=persistent_workers, prefetch_factor = prefetch_factor)\n",
    "swin_multi_test_loader = DataLoader(swin_multi_test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                         pin_memory=True, persistent_workers=persistent_workers, prefetch_factor = prefetch_factor)\n",
    "\n",
    "x,y = next(iter(swin_multi_train_loader))\n",
    "print(\"x batch shape:\", x.shape)  # Should be [batch_size, 3, 224, 224]\n",
    "print(\"y batch shape:\", y.shape)  # Should be [batch_size, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e281f09-ce80-4697-9245-c364b92a79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "#optimizer = optim.Adam(swin_multi.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "optimizer = optim.AdamW(swin_multi.parameters(),\n",
    "                        lr=1e-3,            # learning rate, adjust as needed\n",
    "                        betas=(0.9, 0.999), # momentum parameters\n",
    "                        eps=1e-8,           # term added to improve numerical stability\n",
    "                        weight_decay=0.01)  # decoupled weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77d1e7-8d16-4be4-b478-6f48b49b5d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "swin_multi_train_losses = []\n",
    "swin_multi_val_losses = []\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "    train_loss = AF.training_epoch(swin_multi, swin_multi_train_loader, optimizer, criterion, device, unsqueezeY = False)\n",
    "    val_loss = AF.evaluation_epoch(swin_multi, swin_multi_val_loader, criterion, device, desc = 'validation', unsqueezeY = False)\n",
    "\n",
    "    # Step the scheduler\n",
    "    #scheduler.step()\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    swin_multi_train_losses.append(train_loss)\n",
    "    swin_multi_val_losses.append(val_loss)\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}/{epochs} took {epoch_time:.2f} seconds\")\n",
    "\n",
    "swin_multi_epoch_loss, swin_multi_epoch_acc, swin_multi_all_preds, swin_multi_all_labels = AF.multilabel_evaluate(swin_multi, swin_multi_test_loader,\n",
    "                                                                                                                  criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696d7e6c-5e06-4b96-a1e6-940dc2799d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if epoch+1 == epochs:\n",
    "    ep = epochs\n",
    "else:\n",
    "    ep = epoch\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(np.arange(ep), swin_multi_train_losses, label=\"Training\")\n",
    "ax.plot(np.arange(ep), swin_multi_val_losses, label=\"Validation\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fcbb2c-3a7a-4e8a-92e3-87aa085aca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "af.multilabel_plot_confusion_matrix(swin_multi_all_labels, swin_multi_all_preds, classes=None, normalize=True, title=None, cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020bcf34-1ec3-4614-bec4-bd1e4d79da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_strat_labels(dataset):\n",
    "#     labels = []\n",
    "#     for i in range(len(dataset)):\n",
    "#         _, label = dataset[i]\n",
    "#         label = tuple(label.cpu().numpy())\n",
    "#         labels.append(label)\n",
    "#     return labels\n",
    "\n",
    "# def print_distribution_ratios(labels, subset_name=\"\"):\n",
    "#     counts = Counter(labels)\n",
    "#     total = sum(counts.values())\n",
    "#     print(f\"{subset_name} distribution ratios:\")\n",
    "#     for label, count in counts.items():\n",
    "#         ratio = count / total\n",
    "#         print(f\"  {label}: {ratio:.3f}\")\n",
    "#     print()\n",
    "\n",
    "# orig_labels = get_strat_labels(multidata)\n",
    "# train_labels = get_strat_labels(swin_multi_train_dataset)\n",
    "# val_labels = get_strat_labels(swin_multi_val_dataset)\n",
    "# test_labels = get_strat_labels(swin_multi_test_dataset)\n",
    "\n",
    "# print_distribution_ratios(orig_labels, \"Original\")\n",
    "# print_distribution_ratios(train_labels, \"Train\")\n",
    "# print_distribution_ratios(val_labels, \"Validation\")\n",
    "# print_distribution_ratios(test_labels, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be23819-c594-4669-a443-6995e3628688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
